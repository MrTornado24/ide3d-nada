{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/rinongal/stylegan-nada/blob/StyleGAN3-NADA/stylegan3_nada.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bYsd0_RFXb04"
   },
   "source": [
    "# StyleGAN3-NADA - The Official Unofficial Notebook\n",
    "\n",
    "This is an officialy unofficial re-implementation of StyleGAN-NADA using StyleGAN3.\n",
    "For the implementation used with the StyleGAN-NADA paper please see `stylegan_nada.ipynb` instead.\n",
    "\n",
    "Please note that the SG3 version of the model has not been thoroughly tested, hyperparameters have not been tuned. There is currently no support for inversions. The notebook is likely to change if and when such tools are released.\n",
    "\n",
    "Some modifications can cause the StyleGAN3 'grid' to appear in the generated images. I have done my best to mitigate the most glaring causes of this issue, but removing it completely will require further work."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QTHeOO8qFw_e"
   },
   "source": [
    "# Step 1: Setup required libraries and models. \n",
    "This may take a few minutes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "ph3R7lbl_arQ"
   },
   "outputs": [],
   "source": [
    "#@title Setup\n",
    "import os\n",
    "\n",
    "pretrained_model_dir = os.path.join(\"/content\", \"models\")\n",
    "os.makedirs(pretrained_model_dir, exist_ok=True)\n",
    "\n",
    "stylegan_nada_dir = os.path.join(\"/content\", \"stylegan_nada\")\n",
    "\n",
    "output_dir = os.path.join(\"/content\", \"output\")\n",
    "\n",
    "output_model_dir = os.path.join(output_dir, \"models\")\n",
    "output_image_dir = os.path.join(output_dir, \"images\")\n",
    "\n",
    "def download_model(model_name):\n",
    "    download_url = f\"https://api.ngc.nvidia.com/v2/models/nvidia/research/stylegan3/versions/1/files/{model_name}\" \n",
    "    output_path = f\"{os.path.join(pretrained_model_dir, model_name)}\"\n",
    "\n",
    "    if not os.path.isfile(output_path):\n",
    "        !wget $download_url -O $output_path\n",
    "    else:\n",
    "        print(\"Model already exists. Skipping download...\")\n",
    "    \n",
    "# install requirements\n",
    "!wget https://github.com/ninja-build/ninja/releases/download/v1.8.2/ninja-linux.zip\n",
    "!sudo unzip ninja-linux.zip -d /usr/local/bin/\n",
    "!sudo update-alternatives --install /usr/bin/ninja ninja /usr/local/bin/ninja 1 --force\n",
    "\n",
    "!pip install ftfy regex tqdm \n",
    "!pip install git+https://github.com/openai/CLIP.git\n",
    "\n",
    "!git clone -b StyleGAN3-NADA https://github.com/rinongal/stylegan-nada.git $stylegan_nada_dir\n",
    "\n",
    "from argparse import Namespace\n",
    "\n",
    "import sys\n",
    "import numpy as np\n",
    "\n",
    "from PIL import Image\n",
    "\n",
    "import torch\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "import copy\n",
    "import pickle\n",
    "\n",
    "sys.path.append(stylegan_nada_dir)\n",
    "sys.path.append(os.path.join(stylegan_nada_dir, \"ZSSGAN\"))\n",
    "\n",
    "device = 'cuda'\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kSL166pfGRWF"
   },
   "source": [
    "# Step 2: Choose a model type.\n",
    "\n",
    "Model will be downloaded.\n",
    "\n",
    "Re-runs of the cell with the same model will re-use the previously downloaded version. Feel free to experiment and come back to previous models :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "J4ATNsC1k28g"
   },
   "outputs": [],
   "source": [
    "source_model_type = 'ffhq-t' #@param['ffhq-t', 'ffhq-r', 'metfaces-t', 'metfaces-r', 'afhq-t', 'afhq-r']\n",
    "\n",
    "\n",
    "model_names = {\n",
    "    \"ffhq-t\": \"stylegan3-t-ffhq-1024x1024.pkl\",\n",
    "    \"ffhq-r\": \"stylegan3-r-ffhq-1024x1024.pkl\",\n",
    "    \"metfaces-t\": \"stylegan3-t-metfaces-1024x1024.pkl\",\n",
    "    \"metfaces-u\": \"stylegan3-r-metfaces-1024x1024.pkl\",\n",
    "    \"afhq-t\": \"stylegan3-t-afhqv2-512x512.pkl\",\n",
    "    \"afhq-r\": \"stylegan3-r-afhqv2-512x512.pkl\"\n",
    "}\n",
    "\n",
    "dataset_sizes = {\n",
    "    \"ffhq-t\": 1024,\n",
    "    \"ffhq-r\": 1024,\n",
    "    \"metfaces-t\": 1024,\n",
    "    \"metfaces-u\": 1024,\n",
    "    \"afhq-t\": 512,\n",
    "    \"afhq-r\": 512\n",
    "}\n",
    "\n",
    "model_name = model_names[source_model_type]\n",
    "download_model(model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DAri8ULOG2VE"
   },
   "source": [
    "# Step 3: Train the model.\n",
    "Describe your source and target class. These describe the direction of change you're trying to apply (e.g. \"photo\" to \"sketch\", \"photo\" to \"Fernando Botero painting\" or \"animal\" to \"Nicolas Cage\").\n",
    "\n",
    "Alternatively, upload a directory with a small (~3) set of target style images (there is no need to preprocess them in any way) and set `style_image_dir` to point at them. This will use the images as a target rather than the source/class texts.\n",
    "\n",
    "We find that StyleGAN3 changes typically require 600-800 iterations, and that increasing the batch size to 4 can help mitigate some grid-related artifacts (if you luck out on a GPU that can support the increased batch size)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "8YrtPb7KF8m-"
   },
   "outputs": [],
   "source": [
    "from ZSSGAN.model.ZSSGAN import ZSSGAN\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "\n",
    "from tqdm import notebook\n",
    "\n",
    "from ZSSGAN.utils.file_utils import save_images, get_dir_img_list\n",
    "from ZSSGAN.utils.training_utils import mixing_noise\n",
    "\n",
    "from IPython.display import display\n",
    "\n",
    "source_class = \"Photo\" #@param {\"type\": \"string\"}\n",
    "target_class = \"Sketch\" #@param {\"type\": \"string\"}\n",
    "\n",
    "style_image_dir = \"\" #@param {'type': 'string'}\n",
    "\n",
    "target_img_list = get_dir_img_list(style_image_dir) if style_image_dir else None\n",
    "\n",
    "CLIP_model = \"ViT-B/32 + ViT-B/16\" #@param[\"ViT-B/32\", \"ViT-B/16\", \"ViT-B/32 + ViT-B/16\"]\n",
    "\n",
    "model_choice = [\"ViT-B/32\", \"ViT-B/16\"]\n",
    "model_weights = [0.0, 0.0]\n",
    "\n",
    "if \"ViT-B/32\" in CLIP_model:\n",
    "    model_weights[0] = 1.0\n",
    "if \"ViT-B/16\" in CLIP_model:\n",
    "    model_weights[1] = 1.0\n",
    "\n",
    "training_iterations = 601 #@param {type: \"integer\"}\n",
    "output_interval     = 50 #@param {type: \"integer\"}\n",
    "save_interval       = 0 #@param {type: \"integer\"}\n",
    "\n",
    "training_args = {\n",
    "    \"size\": dataset_sizes[source_model_type],\n",
    "    \"batch\": 2,\n",
    "    \"n_sample\": 4,\n",
    "    \"output_dir\": output_dir,\n",
    "    \"lr\": 0.002,\n",
    "    \"frozen_gen_ckpt\": os.path.join(pretrained_model_dir, model_name),\n",
    "    \"train_gen_ckpt\": os.path.join(pretrained_model_dir, model_name),\n",
    "    \"iter\": training_iterations,\n",
    "    \"source_class\": source_class,\n",
    "    \"target_class\": target_class,\n",
    "    \"lambda_direction\": 1.0,\n",
    "    \"lambda_patch\": 0.0,\n",
    "    \"lambda_global\": 0.0,\n",
    "    \"lambda_texture\": 0.0,\n",
    "    \"lambda_manifold\": 0.0,\n",
    "    \"auto_layer_k\": 0,\n",
    "    \"auto_layer_iters\": 0,\n",
    "    \"auto_layer_batch\": 8,\n",
    "    \"output_interval\": 50,\n",
    "    \"clip_models\": model_choice,\n",
    "    \"clip_model_weights\": model_weights,\n",
    "    \"mixing\": 0.0,\n",
    "    \"phase\": None,\n",
    "    \"sample_truncation\": 0.7,\n",
    "    \"save_interval\": save_interval,\n",
    "    \"target_img_list\": target_img_list,\n",
    "    \"img2img_batch\": 16,\n",
    "    \"sg3\": True,\n",
    "    \"sgxl\", False,\n",
    "}\n",
    "\n",
    "args = Namespace(**training_args)\n",
    "\n",
    "print(\"Loading base models...\")\n",
    "net = ZSSGAN(args)\n",
    "print(\"Models loaded! Starting training...\")\n",
    "\n",
    "g_reg_ratio = 4 / 5\n",
    "\n",
    "g_optim = torch.optim.Adam(\n",
    "    net.generator_trainable.parameters(),\n",
    "    lr=args.lr * g_reg_ratio,\n",
    "    betas=(0 ** g_reg_ratio, 0.99 ** g_reg_ratio),\n",
    ")\n",
    "\n",
    "# Set up output directories.\n",
    "sample_dir = os.path.join(args.output_dir, \"sample\")\n",
    "ckpt_dir   = os.path.join(args.output_dir, \"checkpoint\")\n",
    "\n",
    "os.makedirs(sample_dir, exist_ok=True)\n",
    "os.makedirs(ckpt_dir, exist_ok=True)\n",
    "\n",
    "seed = 3 #@param {\"type\": \"integer\"}\n",
    "\n",
    "torch.manual_seed(seed)\n",
    "np.random.seed(seed)\n",
    "\n",
    "# Training loop\n",
    "fixed_z = torch.randn(args.n_sample, 512, device=device)\n",
    "\n",
    "for i in notebook.tqdm(range(args.iter)):\n",
    "    net.train()\n",
    "        \n",
    "    sample_z = mixing_noise(args.batch, 512, args.mixing, device)\n",
    "\n",
    "    [sampled_src, sampled_dst], clip_loss = net(sample_z)\n",
    "\n",
    "    net.zero_grad()\n",
    "    clip_loss.backward()\n",
    "\n",
    "    g_optim.step()\n",
    "\n",
    "    if i % output_interval == 0:\n",
    "        net.eval()\n",
    "\n",
    "        with torch.no_grad():\n",
    "            [sampled_src, sampled_dst], loss = net([fixed_z], truncation=args.sample_truncation)\n",
    "\n",
    "            grid_rows = 4\n",
    "\n",
    "            save_images(sampled_dst, sample_dir, \"dst\", grid_rows, i)\n",
    "\n",
    "            img = Image.open(os.path.join(sample_dir, f\"dst_{str(i).zfill(6)}.jpg\")).resize((1024, 256))\n",
    "            display(img)\n",
    "    \n",
    "    if (args.save_interval > 0) and (i > 0) and (i % args.save_interval == 0):\n",
    "        snapshot_data = {'G_ema': copy.deepcopy(net.generator_trainable.generator).eval().requires_grad_(False).cpu()}\n",
    "        snapshot_pkl = f'{ckpt_dir}/{str(i).zfill(6)}.pkl'\n",
    "\n",
    "        with open(snapshot_pkl, 'wb') as f:\n",
    "            pickle.dump(snapshot_data, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9ZZk6yZQvxGY"
   },
   "source": [
    "# Step 4: Generate samples with the new model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "dLinyTgev5Qk"
   },
   "outputs": [],
   "source": [
    "truncation = 0.7 #@param {type:\"slider\", min:0, max:1, step:0.05}\n",
    "\n",
    "samples = 9\n",
    "\n",
    "with torch.no_grad():\n",
    "    net.eval()\n",
    "    sample_z = torch.randn(samples, 512, device=device)\n",
    "\n",
    "    [sampled_src, sampled_dst], loss = net([sample_z], truncation=truncation)\n",
    "\n",
    "    grid_rows = int(samples ** 0.5)\n",
    "\n",
    "    save_images(sampled_dst, sample_dir, \"sampled\", grid_rows, 0)\n",
    "\n",
    "    display(Image.open(os.path.join(sample_dir, f\"sampled_{str(0).zfill(6)}.jpg\")).resize((768, 768)))"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyM+XErRWoKRUgyMSyUOOQav",
   "collapsed_sections": [],
   "include_colab_link": true,
   "name": "stylegan_nada.ipynb",
   "provenance": []
  },
  "interpreter": {
   "hash": "fd69f43f58546b570e94fd7eba7b65e6bcc7a5bbc4eab0408017d18902915d69"
  },
  "kernelspec": {
   "display_name": "Python 3.7.5 64-bit",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": ""
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
